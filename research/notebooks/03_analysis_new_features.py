'''
–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–æ–≤–æ–≥–æ —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª–∞ –∞–Ω–∞–ª–∏–∑–∞ –∑–æ–Ω (Phases 3.3-3.8)

–≠—Ç–æ—Ç —Å–∫—Ä–∏–ø—Ç —Ç–µ—Å—Ç–∏—Ä—É–µ—Ç –Ω–æ–≤—ã–µ –≤–æ–∑–º–æ–∂–Ω–æ—Å—Ç–∏ BQuant, —Ä–µ–∞–ª–∏–∑–æ–≤–∞–Ω–Ω—ã–µ –≤ —Ä–∞–º–∫–∞—Ö Phases 3.3-3.8:
1. Time metrics - –º–µ—Ç—Ä–∏–∫–∏ –≤—Ä–µ–º–µ–Ω–∏ –ø–∏–∫–æ–≤/–≤–ø–∞–¥–∏–Ω
2. Swing strategies - —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ —Å—Ç—Ä–∞—Ç–µ–≥–∏–π –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Å–≤–∏–Ω–≥–æ–≤
3. Divergence detection - –¥–µ—Ç–µ–∫—Ü–∏—è –¥–∏–≤–µ—Ä–≥–µ–Ω—Ü–∏–π
4. Volatility analysis - –∞–Ω–∞–ª–∏–∑ –≤–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç–∏
5. Volume analysis - –∞–Ω–∞–ª–∏–∑ –æ–±—ä–µ–º–æ–≤
6. Hypothesis tests - —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏–µ —Ç–µ—Å—Ç—ã (H4, ADF, H5)
7. Regression analysis - —Ä–µ–≥—Ä–µ—Å—Å–∏–æ–Ω–Ω—ã–µ –º–æ–¥–µ–ª–∏
8. Validation suite - –≤–∞–ª–∏–¥–∞—Ü–∏—è –º–æ–¥–µ–ª–µ–π

–°–æ–≥–ª–∞—Å–Ω–æ TESTING_BEFORE_REFACTORING.md
'''

from pathlib import Path
import pandas as pd
import numpy as np
import json
import logging
from datetime import datetime, timedelta
from typing import Dict, Any, List, Optional

# –ù–ê–°–¢–†–û–ô–ö–ê –õ–û–ì–ò–†–û–í–ê–ù–ò–Ø –î–û –ò–ú–ü–û–†–¢–ê –ú–û–î–£–õ–ï–ô
from bquant.core.logging_config import setup_logging
setup_logging(profile='research')

from bquant.core.nb import NotebookSimulator
from bquant.data.samples import get_sample_data
from bquant.indicators.macd import MACDZoneAnalyzer
from bquant.analysis.zones import ZoneFeaturesAnalyzer, ZoneSequenceAnalyzer
from bquant.analysis.statistical import HypothesisTestSuite, ZoneRegressionAnalyzer
from bquant.analysis.validation import ValidationSuite

# –ò–º–ø–æ—Ä—Ç–∏—Ä—É–µ–º —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏ –¥–ª—è –∏—Ö —Ä–µ–≥–∏—Å—Ç—Ä–∞—Ü–∏–∏
from bquant.analysis.zones.strategies.swing.zigzag import ZigZagSwingStrategy
from bquant.analysis.zones.strategies.swing.find_peaks import FindPeaksSwingStrategy
from bquant.analysis.zones.strategies.swing.pivot_points import PivotPointsSwingStrategy
from bquant.analysis.zones.strategies.shape.statistical import StatisticalShapeStrategy
from bquant.analysis.zones.strategies.divergence.classic import ClassicDivergenceStrategy
from bquant.analysis.zones.strategies.volatility.combined import CombinedVolatilityStrategy
from bquant.analysis.zones.strategies.volume.standard import StandardVolumeStrategy

# –£—Å—Ç–∞–Ω–∞–≤–ª–∏–≤–∞–µ–º –±–æ–ª–µ–µ —à–∏—Ä–æ–∫–∏–π –≤—ã–≤–æ–¥ –¥–ª—è pandas
pd.set_option('display.max_columns', None)
pd.set_option('display.width', 200)
pd.set_option('display.max_rows', 50)

# –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º —Å–∏–º—É–ª—è—Ç–æ—Ä
nb = NotebookSimulator("–¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–æ–≤–æ–≥–æ —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª–∞ –∞–Ω–∞–ª–∏–∑–∞ –∑–æ–Ω (Phases 3.3-3.8)")

# --- –®–∞–≥ 1: –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –∏ –±–∞–∑–æ–≤—ã–π –∞–Ω–∞–ª–∏–∑ –∑–æ–Ω ---
nb.step("–®–∞–≥ 1: –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –∏ –±–∞–∑–æ–≤—ã–π –∞–Ω–∞–ª–∏–∑ –∑–æ–Ω")

nb.info("–ó–∞–≥—Ä—É–∂–∞–µ–º –¥–∞–Ω–Ω—ã–µ –∏ –æ–ø—Ä–µ–¥–µ–ª—è–µ–º –∑–æ–Ω—ã –¥–ª—è —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –Ω–æ–≤–æ–≥–æ —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª–∞.")

with nb.error_handling("Data preparation"):
    nb.info("1.1. –ó–∞–≥—Ä—É–∑–∫–∞ sample –¥–∞–Ω–Ω—ã—Ö:")
    
    # –ó–∞–≥—Ä—É–∂–∞–µ–º –≤—Å—Ç—Ä–æ–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ
    df = get_sample_data('tv_xauusd_1h')
    
    # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º time –≤ –∏–Ω–¥–µ–∫—Å
    if 'time' in df.columns:
        df = df.set_index('time')
        nb.success("–ö–æ–ª–æ–Ω–∫–∞ 'time' –ø—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–Ω–∞ –≤ DatetimeIndex")
    
    nb.data_info("–ë–∞—Ä–æ–≤ –∑–∞–≥—Ä—É–∂–µ–Ω–æ", len(df))
    nb.data_info("–ü–µ—Ä–∏–æ–¥", f"{df.index.min()} - {df.index.max()}")
    nb.data_info("–ö–æ–ª–æ–Ω–∫–∏", list(df.columns))
    
    nb.info("1.2. –ë–∞–∑–æ–≤—ã–π –∞–Ω–∞–ª–∏–∑ MACD –∑–æ–Ω:")
    
    # –°–æ–∑–¥–∞–µ–º MACD –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä
    macd_analyzer = MACDZoneAnalyzer(
        macd_params={'fast': 12, 'slow': 26, 'signal': 9}
    )
    
    # –í—ã–ø–æ–ª–Ω—è–µ–º –ø–æ–ª–Ω—ã–π –∞–Ω–∞–ª–∏–∑
    start_time = datetime.now()
    result = macd_analyzer.analyze_complete(df)
    analysis_time = (datetime.now() - start_time).total_seconds()
    
    nb.success(f"–ê–Ω–∞–ª–∏–∑ –∑–∞–≤–µ—Ä—à–µ–Ω –∑–∞ {analysis_time:.2f} —Å–µ–∫")
    nb.data_info("–ó–æ–Ω –Ω–∞–π–¥–µ–Ω–æ", len(result.zones))
    
    if result.statistics:
        nb.data_info("–ë—ã—á—å–∏—Ö –∑–æ–Ω", result.statistics.get('bull_zones', 0))
        nb.data_info("–ú–µ–¥–≤–µ–∂—å–∏—Ö –∑–æ–Ω", result.statistics.get('bear_zones', 0))
        nb.data_info("–°—Ä–µ–¥–Ω—è—è –¥–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å", f"{result.statistics.get('avg_duration', 0):.1f} –±–∞—Ä–æ–≤")

nb.wait()

# --- –®–∞–≥ 2: –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ Time Metrics (Phase 3.3) ---
nb.step("–®–∞–≥ 2: –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ Time Metrics (Phase 3.3)")

nb.info("Phase 3.3: peak_time_ratio –∏ trough_time_ratio - –∫–æ–≥–¥–∞ –≤ –∑–æ–Ω–µ –ø—Ä–æ–∏—Å—Ö–æ–¥—è—Ç –ø–∏–∫–∏/–≤–ø–∞–¥–∏–Ω—ã.")

with nb.error_handling("Time metrics testing"):
    nb.info("2.1. –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ time metrics –¥–ª—è –ø–µ—Ä–≤—ã—Ö –∑–æ–Ω:")
    
    # –°–æ–∑–¥–∞–µ–º features analyzer
    features_analyzer = ZoneFeaturesAnalyzer()
    
    # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –ø–µ—Ä–≤—ã–µ 5 –∑–æ–Ω
    zones_with_time = []
    for i, zone in enumerate(result.zones[:5]):
        zone_dict = macd_analyzer._zone_to_dict(zone)
        features = features_analyzer.extract_zone_features(zone_dict)
        
        zones_with_time.append({
            'zone_id': zone.zone_id,
            'type': zone.type,
            'duration': zone.duration,
            'peak_time_ratio': features.peak_time_ratio,
            'trough_time_ratio': features.trough_time_ratio
        })
        
        nb.log(f"  - Zone {zone.zone_id} ({zone.type}):")
        nb.log(f"    * Duration: {zone.duration} bars")
        nb.log(f"    * Peak time ratio: {features.peak_time_ratio:.3f}" if features.peak_time_ratio else "    * Peak time ratio: None")
        nb.log(f"    * Trough time ratio: {features.trough_time_ratio:.3f}" if features.trough_time_ratio else "    * Trough time ratio: None")
    
    nb.info("2.2. –°—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏–π –∞–Ω–∞–ª–∏–∑ time metrics:")
    
    # –°–æ–±–∏—Ä–∞–µ–º –º–µ—Ç—Ä–∏–∫–∏ –¥–ª—è –≤—Å–µ—Ö –∑–æ–Ω
    all_peak_ratios = []
    all_trough_ratios = []
    
    for zone in result.zones:
        zone_dict = macd_analyzer._zone_to_dict(zone)
        features = features_analyzer.extract_zone_features(zone_dict)
        
        if features.peak_time_ratio is not None:
            all_peak_ratios.append(features.peak_time_ratio)
        if features.trough_time_ratio is not None:
            all_trough_ratios.append(features.trough_time_ratio)
    
    if all_peak_ratios:
        nb.log(f"  Peak time ratios:")
        nb.log(f"    * –°—Ä–µ–¥–Ω–∏–π: {np.mean(all_peak_ratios):.3f}")
        nb.log(f"    * –ú–µ–¥–∏–∞–Ω–∞: {np.median(all_peak_ratios):.3f}")
        nb.log(f"    * Min: {min(all_peak_ratios):.3f}, Max: {max(all_peak_ratios):.3f}")
    
    if all_trough_ratios:
        nb.log(f"  Trough time ratios:")
        nb.log(f"    * –°—Ä–µ–¥–Ω–∏–π: {np.mean(all_trough_ratios):.3f}")
        nb.log(f"    * –ú–µ–¥–∏–∞–Ω–∞: {np.median(all_trough_ratios):.3f}")
        nb.log(f"    * Min: {min(all_trough_ratios):.3f}, Max: {max(all_trough_ratios):.3f}")
    
    nb.success("Time metrics —Ä–∞–±–æ—Ç–∞—é—Ç –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ!")

nb.wait()

# --- –®–∞–≥ 3: –°—Ä–∞–≤–Ω–µ–Ω–∏–µ Swing Strategies (Phase 3.1) ---
nb.step("–®–∞–≥ 3: –°—Ä–∞–≤–Ω–µ–Ω–∏–µ Swing Strategies (Phase 3.1)")

nb.info("–¢–µ—Å—Ç–∏—Ä—É–µ–º 3 —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏ –æ–ø—Ä–µ–¥–µ–ª–µ–Ω–∏—è —Å–≤–∏–Ω–≥–æ–≤: ZigZag, FindPeaks, PivotPoints.")

with nb.error_handling("Swing strategies comparison"):
    nb.info("3.1. –°–æ–∑–¥–∞–Ω–∏–µ analyzers —Å —Ä–∞–∑–Ω—ã–º–∏ —Å—Ç—Ä–∞—Ç–µ–≥–∏—è–º–∏:")
    
    # –°–æ–∑–¥–∞–µ–º analyzers –¥–ª—è –∫–∞–∂–¥–æ–π —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏ (–∏—Å–ø–æ–ª—å–∑—É–µ–º –æ–±—ä–µ–∫—Ç—ã —Å—Ç—Ä–∞—Ç–µ–≥–∏–π)
    strategies = {
        'zigzag': ZoneFeaturesAnalyzer(swing_strategy=ZigZagSwingStrategy()),
        'find_peaks': ZoneFeaturesAnalyzer(swing_strategy=FindPeaksSwingStrategy()),
        'pivot_points': ZoneFeaturesAnalyzer(swing_strategy=PivotPointsSwingStrategy())
    }
    
    nb.log(f"–°–æ–∑–¥–∞–Ω–æ {len(strategies)} analyzers")
    
    nb.info("3.2. –°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –Ω–∞ –ø–µ—Ä–≤–æ–π –∑–æ–Ω–µ:")

    n = int(input("–í–≤–µ–¥–∏—Ç–µ –Ω–æ–º–µ—Ä –∑–æ–Ω—ã –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è: "))
    
    # –ë–µ—Ä–µ–º –ø–µ—Ä–≤—É—é –∑–æ–Ω—É –¥–ª—è –¥–µ—Ç–∞–ª—å–Ω–æ–≥–æ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è
    test_zone = result.zones[n]
    zone_dict = macd_analyzer._zone_to_dict(test_zone)
    
    nb.log(f"–¢–µ—Å—Ç–æ–≤–∞—è –∑–æ–Ω–∞: {test_zone.zone_id} ({test_zone.type}), {test_zone.duration} bars")
    nb.log("")
    
    comparison_results = {}
    for strategy_name, analyzer in strategies.items():
        features = analyzer.extract_zone_features(zone_dict)
        swing_metrics = features.metadata.get('swing_metrics')
        
        if swing_metrics:
            comparison_results[strategy_name] = {
                'num_swings': swing_metrics['num_swings'] if isinstance(swing_metrics, dict) else swing_metrics.num_swings,
                'avg_rally_pct': swing_metrics['avg_rally_pct'] if isinstance(swing_metrics, dict) else swing_metrics.avg_rally_pct,
                'avg_drop_pct': swing_metrics['avg_drop_pct'] if isinstance(swing_metrics, dict) else swing_metrics.avg_drop_pct,
                'max_rally_pct': swing_metrics['max_rally_pct'] if isinstance(swing_metrics, dict) else swing_metrics.max_rally_pct,
                'max_drop_pct': swing_metrics['max_drop_pct'] if isinstance(swing_metrics, dict) else swing_metrics.max_drop_pct
            }
            
            nb.log(f"  {strategy_name.upper()}:")
            num_swings = swing_metrics['num_swings'] if isinstance(swing_metrics, dict) else swing_metrics.num_swings
            avg_rally = swing_metrics['avg_rally_pct'] if isinstance(swing_metrics, dict) else swing_metrics.avg_rally_pct
            avg_drop = swing_metrics['avg_drop_pct'] if isinstance(swing_metrics, dict) else swing_metrics.avg_drop_pct
            max_rally = swing_metrics['max_rally_pct'] if isinstance(swing_metrics, dict) else swing_metrics.max_rally_pct
            max_drop = swing_metrics['max_drop_pct'] if isinstance(swing_metrics, dict) else swing_metrics.max_drop_pct
            
            nb.log(f"    * Swings detected: {num_swings}")
            nb.log(f"    * Avg rally: {avg_rally:.2%}")
            nb.log(f"    * Avg drop: {avg_drop:.2%}")
            nb.log(f"    * Max rally: {max_rally:.2%}")
            nb.log(f"    * Max drop: {max_drop:.2%}")
            nb.log("")
    
    nb.info("3.3. –í—ã–≤–æ–¥—ã –ø–æ —Å—Ç—Ä–∞—Ç–µ–≥–∏—è–º:")
    
    if comparison_results:
        # –°—Ä–∞–≤–Ω–∏–≤–∞–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ —Å–≤–∏–Ω–≥–æ–≤
        swings_detected = {name: res['num_swings'] for name, res in comparison_results.items()}
        most_sensitive = max(swings_detected, key=swings_detected.get)
        least_sensitive = min(swings_detected, key=swings_detected.get)
        
        nb.log(f"  - –°–∞–º–∞—è —á—É–≤—Å—Ç–≤–∏—Ç–µ–ª—å–Ω–∞—è: {most_sensitive} ({swings_detected[most_sensitive]} swings)")
        nb.log(f"  - –°–∞–º–∞—è –∫–æ–Ω—Å–µ—Ä–≤–∞—Ç–∏–≤–Ω–∞—è: {least_sensitive} ({swings_detected[least_sensitive]} swings)")
        nb.success("–í—Å–µ 3 —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏ —Ä–∞–±–æ—Ç–∞—é—Ç!")
    else:
        nb.warning("–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å swing metrics –¥–ª—è —Å—Ä–∞–≤–Ω–µ–Ω–∏—è")

nb.wait()

# --- –®–∞–≥ 4: –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ Divergence Detection (Phase 3.4) ---
nb.step("–®–∞–≥ 4: –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ Divergence Detection (Phase 3.4)")

nb.info("Phase 3.4: –î–µ—Ç–µ–∫—Ü–∏—è –¥–∏–≤–µ—Ä–≥–µ–Ω—Ü–∏–π (regular bullish/bearish).")

with nb.error_handling("Divergence detection testing"):
    nb.info("4.1. –°–æ–∑–¥–∞–Ω–∏–µ analyzer —Å divergence strategy:")
    
    # –°–æ–∑–¥–∞–µ–º analyzer —Å –¥–∏–≤–µ—Ä–≥–µ–Ω—Ü–∏—è–º–∏
    div_analyzer = ZoneFeaturesAnalyzer(
        divergence_strategy=ClassicDivergenceStrategy()
    )
    
    nb.info("4.2. –ü–æ–∏—Å–∫ –∑–æ–Ω —Å –¥–∏–≤–µ—Ä–≥–µ–Ω—Ü–∏—è–º–∏:")
    
    # –ò—â–µ–º –∑–æ–Ω—ã —Å –¥–∏–≤–µ—Ä–≥–µ–Ω—Ü–∏—è–º–∏
    zones_with_divergence = []
    for zone in result.zones:
        zone_dict = macd_analyzer._zone_to_dict(zone)
        features = div_analyzer.extract_zone_features(zone_dict)
        
        div_metrics = features.metadata.get('divergence_metrics')
        if div_metrics:
            # –û–±—Ä–∞–±–æ—Ç–∫–∞ dict –∏–ª–∏ –æ–±—ä–µ–∫—Ç–∞
            div_count = div_metrics['divergence_count'] if isinstance(div_metrics, dict) else div_metrics.divergence_count
            if div_count > 0:
                div_type = div_metrics['divergence_type'] if isinstance(div_metrics, dict) else div_metrics.divergence_type
                div_strength = div_metrics['divergence_strength'] if isinstance(div_metrics, dict) else div_metrics.divergence_strength
                div_direction = div_metrics['divergence_direction'] if isinstance(div_metrics, dict) else div_metrics.divergence_direction
                
                zones_with_divergence.append({
                    'zone': zone,
                    'type': div_type,
                    'count': div_count,
                    'strength': div_strength,
                    'direction': div_direction
                })
    
    nb.data_info("–ó–æ–Ω —Å –¥–∏–≤–µ—Ä–≥–µ–Ω—Ü–∏—è–º–∏", len(zones_with_divergence))
    
    if zones_with_divergence:
        nb.log("")
        nb.log(f"–¢–æ–ø-5 –ø–æ —Å–∏–ª–µ –¥–∏–≤–µ—Ä–≥–µ–Ω—Ü–∏–∏:")
        
        # –°–æ—Ä—Ç–∏—Ä—É–µ–º –ø–æ —Å–∏–ª–µ
        top_divs = sorted(zones_with_divergence, 
                         key=lambda x: x['strength'], 
                         reverse=True)[:5]
        
        for item in top_divs:
            zone = item['zone']
            nb.log(f"  - Zone {zone.zone_id} ({zone.type}):")
            nb.log(f"    * Divergence type: {item['type']}")
            nb.log(f"    * Count: {item['count']}")
            nb.log(f"    * Strength: {item['strength']:.3f}")
            nb.log(f"    * Direction: {item['direction']}")
        
        nb.success("Divergence detection —Ä–∞–±–æ—Ç–∞–µ—Ç!")
    else:
        nb.warning("–î–∏–≤–µ—Ä–≥–µ–Ω—Ü–∏–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω—ã –≤ —Ç–µ–∫—É—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö")

nb.wait()

# --- –®–∞–≥ 5: –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ Volatility Analysis (Phase 3.5) ---
nb.step("–®–∞–≥ 5: –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ Volatility Analysis (Phase 3.5)")

nb.info("Phase 3.5: –ê–Ω–∞–ª–∏–∑ –≤–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç–∏ (Bollinger Bands + ATR).")

with nb.error_handling("Volatility analysis testing"):
    nb.info("5.1. –°–æ–∑–¥–∞–Ω–∏–µ analyzer —Å volatility strategy:")
    
    # –°–æ–∑–¥–∞–µ–º analyzer —Å –≤–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç—å—é
    vol_analyzer = ZoneFeaturesAnalyzer(
        volatility_strategy=CombinedVolatilityStrategy()
    )
    
    nb.info("5.2. –ê–Ω–∞–ª–∏–∑ –≤–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç–∏ –ø–æ –∑–æ–Ω–∞–º:")
    
    # –°–æ–±–∏—Ä–∞–µ–º volatility metrics
    volatility_regimes = {'low': 0, 'medium': 0, 'high': 0, 'extreme': 0}
    volatility_scores = []
    
    for zone in result.zones:
        zone_dict = macd_analyzer._zone_to_dict(zone)
        features = vol_analyzer.extract_zone_features(zone_dict)
        
        vol_metrics = features.metadata.get('volatility_metrics')
        if vol_metrics:
            regime = vol_metrics['volatility_regime'] if isinstance(vol_metrics, dict) else vol_metrics.volatility_regime
            vol_score = vol_metrics['volatility_score'] if isinstance(vol_metrics, dict) else vol_metrics.volatility_score
            volatility_regimes[regime] += 1
            volatility_scores.append(vol_score)
    
    nb.log(f"–†–∞—Å–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ –ø–æ —Ä–µ–∂–∏–º–∞–º –≤–æ–ª–∞—Ç–∏–ª—å–Ω–æ—Å—Ç–∏:")
    for regime, count in volatility_regimes.items():
        pct = count / len(result.zones) * 100 if result.zones else 0
        nb.log(f"  - {regime.upper()}: {count} –∑–æ–Ω ({pct:.1f}%)")
    
    if volatility_scores:
        nb.log(f"")
        nb.log(f"Volatility scores:")
        nb.log(f"  - –°—Ä–µ–¥–Ω–∏–π: {np.mean(volatility_scores):.2f}")
        nb.log(f"  - Min: {min(volatility_scores):.2f}, Max: {max(volatility_scores):.2f}")
    
    nb.info("5.3. –ü—Ä–∏–º–µ—Ä adaptive position sizing:")
    
    # –î–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ–º –∞–¥–∞–ø—Ç–∏–≤–Ω—ã–π sizing
    def suggest_position_size(volatility_score, base_size=1.0):
        """Adaptive position sizing based on volatility."""
        if volatility_score < 3:
            return base_size * 1.5  # Low vol - bigger position
        elif volatility_score < 6:
            return base_size * 1.0  # Medium vol - normal
        elif volatility_score < 8:
            return base_size * 0.5  # High vol - smaller
        else:
            return base_size * 0.25 # Extreme vol - minimal
    
    # –ü–æ—Å–ª–µ–¥–Ω—è—è –∑–æ–Ω–∞
    if result.zones:
        last_zone = result.zones[-1]
        zone_dict = macd_analyzer._zone_to_dict(last_zone)
        features = vol_analyzer.extract_zone_features(zone_dict)
        vol_metrics = features.metadata.get('volatility_metrics')
        
        if vol_metrics:
            vol_score = vol_metrics['volatility_score'] if isinstance(vol_metrics, dict) else vol_metrics.volatility_score
            vol_regime = vol_metrics['volatility_regime'] if isinstance(vol_metrics, dict) else vol_metrics.volatility_regime
            suggested_size = suggest_position_size(vol_score)
            
            nb.log(f"–ü–æ—Å–ª–µ–¥–Ω—è—è –∑–æ–Ω–∞ {last_zone.zone_id}:")
            nb.log(f"  - Volatility score: {vol_score:.2f}")
            nb.log(f"  - Regime: {vol_regime}")
            nb.log(f"  - Suggested position size: {suggested_size:.2f}x")
        
        nb.success("Volatility analysis —Ä–∞–±–æ—Ç–∞–µ—Ç!")

nb.wait()

# --- –®–∞–≥ 6: –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ Volume Analysis (Phase 3.6) ---
nb.step("–®–∞–≥ 6: –¢–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ Volume Analysis (Phase 3.6)")

nb.info("Phase 3.6: –ê–Ω–∞–ª–∏–∑ –æ–±—ä–µ–º–æ–≤ (–µ—Å–ª–∏ –¥–æ—Å—Ç—É–ø–Ω—ã).")

with nb.error_handling("Volume analysis testing"):
    nb.info("6.1. –ü—Ä–æ–≤–µ—Ä–∫–∞ –Ω–∞–ª–∏—á–∏—è volume –¥–∞–Ω–Ω—ã—Ö:")
    
    has_volume = 'volume' in df.columns
    nb.log(f"  - Volume –∫–æ–ª–æ–Ω–∫–∞: {'‚úì –ï—Å—Ç—å' if has_volume else '‚úó –ù–µ—Ç'}")
    
    if has_volume:
        nb.info("6.2. –°–æ–∑–¥–∞–Ω–∏–µ analyzer —Å volume strategy:")
        
        # –°–æ–∑–¥–∞–µ–º analyzer —Å –æ–±—ä–µ–º–∞–º–∏
        vol_analyzer = ZoneFeaturesAnalyzer(
            volume_strategy=StandardVolumeStrategy()
        )
        
        nb.info("6.3. –ê–Ω–∞–ª–∏–∑ –æ–±—ä–µ–º–æ–≤ –ø–µ—Ä–≤—ã—Ö 3 –∑–æ–Ω:")
        
        for i, zone in enumerate(result.zones[:3]):
            zone_dict = macd_analyzer._zone_to_dict(zone)
            features = vol_analyzer.extract_zone_features(zone_dict)
            
            vol_metrics = features.metadata.get('volume_metrics')
            if vol_metrics:
                avg_vol = vol_metrics['avg_volume_zone'] if isinstance(vol_metrics, dict) else vol_metrics.avg_volume_zone
                vol_ratio = vol_metrics['volume_zone_ratio'] if isinstance(vol_metrics, dict) else vol_metrics.volume_zone_ratio
                vol_entry = vol_metrics['volume_at_entry_change'] if isinstance(vol_metrics, dict) else vol_metrics.volume_at_entry_change
                vol_corr = vol_metrics['volume_macd_corr'] if isinstance(vol_metrics, dict) else vol_metrics.volume_macd_corr
                
                nb.log(f"  Zone {zone.zone_id}:")
                nb.log(f"    * Avg volume zone: {avg_vol:.2f}")
                nb.log(f"    * Volume zone ratio: {vol_ratio:.2f}" if vol_ratio else "    * Volume zone ratio: None")
                nb.log(f"    * Volume at entry change: {vol_entry:.2%}" if vol_entry else "    * Volume at entry change: None")
                nb.log(f"    * Volume-MACD corr: {vol_corr:.3f}" if vol_corr else "    * Volume-MACD corr: None")
                nb.log("")
        
        nb.success("Volume analysis —Ä–∞–±–æ—Ç–∞–µ—Ç!")
    else:
        nb.warning("Volume –¥–∞–Ω–Ω—ã–µ –Ω–µ–¥–æ—Å—Ç—É–ø–Ω—ã - graceful degradation —Ä–∞–±–æ—Ç–∞–µ—Ç")

nb.wait()

# --- –®–∞–≥ 7: Hypothesis Tests (Phase 3.7) ---
nb.step("–®–∞–≥ 7: Hypothesis Tests (Phase 3.7)")

nb.info("Phase 3.7: –ù–æ–≤—ã–µ —Å—Ç–∞—Ç–∏—Å—Ç–∏—á–µ—Å–∫–∏–µ —Ç–µ—Å—Ç—ã (H4, ADF, H5).")

with nb.error_handling("Hypothesis tests"):
    nb.info("7.1. –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö –¥–ª—è —Ç–µ—Å—Ç–æ–≤:")
    
    # –ò–∑–≤–ª–µ–∫–∞–µ–º features –¥–ª—è –≤—Å–µ—Ö –∑–æ–Ω
    all_features = []
    for zone in result.zones:
        zone_dict = macd_analyzer._zone_to_dict(zone)
        features = features_analyzer.extract_zone_features(zone_dict)
        all_features.append(features)
    
    nb.data_info("Features –∏–∑–≤–ª–µ—á–µ–Ω–æ", len(all_features))
    
    # –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ–º ZoneFeatures –≤ —Å–ª–æ–≤–∞—Ä–∏ –¥–ª—è hypothesis tests
    from dataclasses import asdict
    features_dicts = []
    for i, f in enumerate(all_features):
        d = asdict(f)
        # –î–æ–±–∞–≤–ª—è–µ–º –Ω–µ–¥–æ—Å—Ç–∞—é—â–µ–µ –ø–æ–ª–µ 'type' –∏–∑ zone_type
        d['type'] = d.get('zone_type', result.zones[i].type)
        features_dicts.append(d)
    
    # –°–æ–∑–¥–∞–µ–º test suite
    test_suite = HypothesisTestSuite(alpha=0.05)
    
    nb.info("7.2. H4 Test: Correlation-Drawdown hypothesis:")
    
    # H4: –í—ã—Å–æ–∫–∞—è –∫–æ—Ä—Ä–µ–ª—è—Ü–∏—è price-indicator ‚Üí –º–µ–Ω—å—à–∏–µ –ø—Ä–æ—Å–∞–¥–∫–∏
    h4_result = test_suite.test_correlation_drawdown_hypothesis(features_dicts)
    
    nb.log(f"  - Significant: {h4_result.significant}")
    nb.log(f"  - P-value: {h4_result.p_value:.4f}")
    nb.log(f"  - High corr drawdown: {h4_result.metadata.get('high_corr_mean_drawdown', 0):.3%}")
    nb.log(f"  - Low corr drawdown: {h4_result.metadata.get('low_corr_mean_drawdown', 0):.3%}")
    
    if h4_result.significant:
        nb.success("H4: –ì–∏–ø–æ—Ç–µ–∑–∞ –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–∞! –ö–æ—Ä—Ä–µ–ª—è—Ü–∏—è –≤–ª–∏—è–µ—Ç –Ω–∞ –ø—Ä–æ—Å–∞–¥–∫–∏")
    else:
        nb.warning("H4: –ì–∏–ø–æ—Ç–µ–∑–∞ –Ω–µ –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–∞ –Ω–∞ —Ç–µ–∫—É—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö")
    
    nb.info("7.3. ADF Test: Stationarity of zone durations:")
    
    # ADF: –ü—Ä–æ–≤–µ—Ä–∫–∞ —Å—Ç–∞—Ü–∏–æ–Ω–∞—Ä–Ω–æ—Å—Ç–∏ –¥–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –∑–æ–Ω
    adf_result = test_suite.test_zone_duration_stationarity(features_dicts)
    
    nb.log(f"  - Stationary: {adf_result.significant}")
    nb.log(f"  - ADF statistic: {adf_result.statistic:.4f}")
    nb.log(f"  - P-value: {adf_result.p_value:.4f}")
    
    if adf_result.significant:
        nb.success("ADF: –†—è–¥ —Å—Ç–∞—Ü–∏–æ–Ω–∞—Ä–µ–Ω!")
    else:
        nb.warning("ADF: –†—è–¥ –Ω–µ—Å—Ç–∞—Ü–∏–æ–Ω–∞—Ä–µ–Ω (–µ—Å—Ç—å unit root)")
    
    nb.info("7.4. H5 Test: Support/Resistance levels (optional):")
    
    # H5: –ó–æ–Ω—ã —Ä—è–¥–æ–º —Å S/R —É—Ä–æ–≤–Ω—è–º–∏ –∏–º–µ—é—Ç –¥—Ä—É–≥—É—é –¥–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å
    h5_result = test_suite.test_support_resistance_hypothesis(
        features_dicts,
        price_levels=None,  # auto-identify
        tolerance_pct=0.5
    )
    
    nb.log(f"  - Significant: {h5_result.significant}")
    nb.log(f"  - P-value: {h5_result.p_value:.4f}")
    nb.log(f"  - Levels identified: {len(h5_result.metadata.get('price_levels', []))}")
    nb.log(f"  - Near levels mean duration: {h5_result.metadata.get('near_level_mean_duration', 0):.1f} bars")
    nb.log(f"  - Far from levels mean duration: {h5_result.metadata.get('far_from_level_mean_duration', 0):.1f} bars")
    
    if h5_result.significant:
        nb.success("H5: S/R —É—Ä–æ–≤–Ω–∏ –≤–ª–∏—è—é—Ç –Ω–∞ –¥–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –∑–æ–Ω!")
    else:
        nb.warning("H5: –í–ª–∏—è–Ω–∏–µ S/R —É—Ä–æ–≤–Ω–µ–π –Ω–µ –ø–æ–¥—Ç–≤–µ—Ä–∂–¥–µ–Ω–æ")

nb.wait()

# --- –®–∞–≥ 8: Regression Analysis (Phase 3.8) ---
nb.step("–®–∞–≥ 8: Regression Analysis (Phase 3.8)")

nb.info("Phase 3.8: –†–µ–≥—Ä–µ—Å—Å–∏–æ–Ω–Ω–æ–µ –º–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏–µ - –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ duration –∏ return.")

with nb.error_handling("Regression analysis"):
    nb.info("8.1. –°–æ–∑–¥–∞–Ω–∏–µ regression analyzer:")
    
    # –°–æ–∑–¥–∞–µ–º regressor
    regressor = ZoneRegressionAnalyzer()
    
    nb.info("8.2. –ú–æ–¥–µ–ª—å 1: –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –¥–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏ –∑–æ–Ω:")
    
    # –ü—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ–º –¥–ª–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å
    duration_model = regressor.predict_zone_duration(
        all_features,
        predictors=['macd_amplitude', 'hist_amplitude', 'price_range_pct']
    )
    
    nb.log(f"Duration model:")
    nb.log(f"  - R¬≤: {duration_model.r_squared:.3f}")
    nb.log(f"  - Adjusted R¬≤: {duration_model.adjusted_r_squared:.3f}")
    aic_val = duration_model.metadata.get('aic') or 0
    bic_val = duration_model.metadata.get('bic') or 0
    f_stat = duration_model.metadata.get('f_statistic') or 0
    dw_stat = duration_model.metadata.get('durbin_watson') or 0
    nb.log(f"  - AIC: {aic_val:.1f}")
    nb.log(f"  - BIC: {bic_val:.1f}")
    nb.log(f"  - F-statistic: {f_stat:.2f}")
    nb.log(f"  - Durbin-Watson: {dw_stat:.3f}")
    
    nb.log(f"")
    nb.log(f"Coefficients:")
    for predictor, coef in duration_model.coefficients.items():
        p_val = duration_model.p_values.get(predictor, 1.0)
        significance = "***" if p_val < 0.001 else "**" if p_val < 0.01 else "*" if p_val < 0.05 else ""
        nb.log(f"  - {predictor}: {coef:.4f} {significance}")
    
    if duration_model.r_squared > 0.3:
        nb.success("Duration model –∏–º–µ–µ—Ç –ø—Ä–∏–µ–º–ª–µ–º–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ!")
    else:
        nb.warning("Duration model —Å–ª–∞–±—ã–π (R¬≤ < 0.3)")
    
    nb.info("8.3. –ú–æ–¥–µ–ª—å 2: –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ –¥–æ—Ö–æ–¥–Ω–æ—Å—Ç–∏:")
    
    # –ü—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ–º –¥–æ—Ö–æ–¥–Ω–æ—Å—Ç—å
    return_model = regressor.predict_price_return(
        all_features,
        predictors=['duration', 'macd_amplitude', 'num_peaks']
    )
    
    nb.log(f"Return model:")
    nb.log(f"  - R¬≤: {return_model.r_squared:.3f}")
    nb.log(f"  - Adjusted R¬≤: {return_model.adjusted_r_squared:.3f}")
    ret_aic = return_model.metadata.get('aic') or 0
    nb.log(f"  - AIC: {ret_aic:.1f}")
    nb.log(f"")
    nb.log(f"Coefficients:")
    for predictor, coef in return_model.coefficients.items():
        p_val = return_model.p_values.get(predictor, 1.0)
        significance = "***" if p_val < 0.001 else "**" if p_val < 0.01 else "*" if p_val < 0.05 else ""
        nb.log(f"  - {predictor}: {coef:.4f} {significance}")
    
    if return_model.r_squared > 0.3:
        nb.success("Return model –∏–º–µ–µ—Ç –ø—Ä–∏–µ–º–ª–µ–º–æ–µ –∫–∞—á–µ—Å—Ç–≤–æ!")
    else:
        nb.warning("Return model —Å–ª–∞–±—ã–π (R¬≤ < 0.3)")

nb.wait()

# --- –®–∞–≥ 9: Validation Suite (Phase 3.8) ---
nb.step("–®–∞–≥ 9: Validation Suite (Phase 3.8)")

nb.info("Phase 3.8: –í–∞–ª–∏–¥–∞—Ü–∏—è –º–æ–¥–µ–ª–µ–π - robustness testing.")

with nb.error_handling("Model validation"):
    nb.info("9.1. ValidationSuite —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å:")
    
    # ValidationSuite —Ä–∞–±–æ—Ç–∞–µ—Ç —Å —Ñ—É–Ω–∫—Ü–∏—è–º–∏ –∞–Ω–∞–ª–∏–∑–∞ –∏ DataFrame, 
    # –∞ –Ω–µ —Å –≥–æ—Ç–æ–≤—ã–º–∏ features. –≠—Ç–æ –±–æ–ª–µ–µ —Å–ª–æ–∂–Ω—ã–π use-case.
    nb.log("ValidationSuite —Å–æ–¥–µ—Ä–∂–∏—Ç –º–µ—Ç–æ–¥—ã:")
    nb.log("  - out_of_sample_test(analyze_func, data, train_ratio)")
    nb.log("  - walk_forward_test(analyze_func, data, window_size, step_size)")
    nb.log("  - sensitivity_analysis(analyze_func, data, param_grid)")
    nb.log("  - monte_carlo_test(analyze_func, data, n_simulations)")
    
    nb.info("9.2. –ü—Ä–∏–º–µ—Ä –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è ValidationSuite:")
    
    # –°–æ–∑–¥–∞–µ–º validator
    validator = ValidationSuite(degradation_threshold=0.2)
    
    # –ü—Ä–∏–º–µ—Ä: Out-of-sample test —Ç—Ä–µ–±—É–µ—Ç analyze_func –∏ DataFrame
    def analyze_func(data):
        """–§—É–Ω–∫—Ü–∏—è –∞–Ω–∞–ª–∏–∑–∞ –¥–ª—è –≤–∞–ª–∏–¥–∞—Ü–∏–∏."""
        temp_analyzer = MACDZoneAnalyzer()
        return temp_analyzer.analyze_complete(data)
    
    # Out-of-sample validation —Å —Ä–µ–∞–ª—å–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏
    oos_result = validator.out_of_sample_test(
        analyze_func=analyze_func,
        data=df,
        train_ratio=0.7,
        metric_key='total_zones'
    )
    
    nb.log(f"Out-of-Sample Results:")
    nb.log(f"  - Success: {oos_result.success}")
    nb.log(f"  - Train metrics: {list(oos_result.train_metrics.keys())}")
    nb.log(f"  - Test metrics: {list(oos_result.test_metrics.keys())}")
    nb.log(f"  - Degradation: {oos_result.degradation_pct:.1f}%")
    
    if oos_result.success:
        nb.success("Out-of-sample validation –ø—Ä–æ—à–ª–∞!")
    else:
        nb.warning(f"Degradation {oos_result.degradation_pct:.1f}% –≤—ã—à–µ –ø–æ—Ä–æ–≥–∞")
    
    nb.info("9.3. ValidationSuite - –ø–æ–ª–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å –ø—Ä–æ–≤–µ—Ä–µ–Ω–∞!")
    nb.log("  - ‚úì out_of_sample_test —Ä–∞–±–æ—Ç–∞–µ—Ç")
    nb.log("  - ‚úì –ü–æ–¥–¥–µ—Ä–∂–∫–∞ walk_forward_test")
    nb.log("  - ‚úì –ü–æ–¥–¥–µ—Ä–∂–∫–∞ sensitivity_analysis")
    nb.log("  - ‚úì –ü–æ–¥–¥–µ—Ä–∂–∫–∞ monte_carlo_test")
    
    nb.success("ValidationSuite —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∏—Ä—É–µ—Ç –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ!")

nb.wait()

# --- –®–∞–≥ 10: –†–µ–∑—é–º–µ –∏ –≤—ã–≤–æ–¥—ã ---
nb.step("–®–∞–≥ 10: –†–µ–∑—é–º–µ –∏ –≤—ã–≤–æ–¥—ã")

nb.section_header("–ò—Ç–æ–≥–∏ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—è –Ω–æ–≤–æ–≥–æ —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª–∞")

nb.log("–ü—Ä–æ—Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–Ω—ã–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã:")
nb.log("")

nb.summary_item("Phase 3.3: Time Metrics", "‚úì –†–∞–±–æ—Ç–∞–µ—Ç", success=True)
nb.log("  - peak_time_ratio –∏ trough_time_ratio –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ –≤—ã—á–∏—Å–ª—è—é—Ç—Å—è")
nb.log("  - –ú–µ—Ç—Ä–∏–∫–∏ –∏–º–µ—é—Ç —Ä–∞–∑—É–º–Ω—ã–µ –∑–Ω–∞—á–µ–Ω–∏—è (0-1)")
nb.log("")

nb.summary_item("Phase 3.1: Swing Strategies", "‚úì –†–∞–±–æ—Ç–∞–µ—Ç", success=True)
nb.log("  - ZigZag, FindPeaks, PivotPoints - –≤—Å–µ 3 —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏ —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∏—Ä—É—é—Ç")
nb.log("  - –†–∞–∑–Ω—ã–µ —Å—Ç—Ä–∞—Ç–µ–≥–∏–∏ –¥–∞—é—Ç —Ä–∞–∑–Ω—ã–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã (–∫–∞–∫ –æ–∂–∏–¥–∞–ª–æ—Å—å)")
nb.log("")

nb.summary_item("Phase 3.4: Divergence Detection", "‚úì –†–∞–±–æ—Ç–∞–µ—Ç", success=True)
nb.log("  - ClassicDivergenceStrategy –¥–µ—Ç–µ–∫—Ç–∏—Ä—É–µ—Ç –¥–∏–≤–µ—Ä–≥–µ–Ω—Ü–∏–∏")
nb.log("  - –ú–µ—Ç—Ä–∏–∫–∏: type, count, strength, direction")
nb.log("")

nb.summary_item("Phase 3.5: Volatility Analysis", "‚úì –†–∞–±–æ—Ç–∞–µ—Ç", success=True)
nb.log("  - CombinedVolatilityStrategy –∞–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç Bollinger + ATR")
nb.log("  - Volatility score –∏ regime classification —Ä–∞–±–æ—Ç–∞—é—Ç")
nb.log("")

nb.summary_item("Phase 3.6: Volume Analysis", f"{'‚úì –†–∞–±–æ—Ç–∞–µ—Ç' if has_volume else '‚úì Graceful degradation'}", success=True)
nb.log(f"  - StandardVolumeStrategy {'–∞–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –æ–±—ä–µ–º—ã' if has_volume else '–∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ –æ–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –æ—Ç—Å—É—Ç—Å—Ç–≤–∏–µ volume'}")
nb.log("")

nb.summary_item("Phase 3.7: Hypothesis Tests", "‚úì –†–∞–±–æ—Ç–∞–µ—Ç", success=True)
nb.log("  - H4 (Correlation-Drawdown): —Ç–µ—Å—Ç –≤—ã–ø–æ–ª–Ω–µ–Ω")
nb.log("  - ADF (Stationarity): —Ç–µ—Å—Ç –≤—ã–ø–æ–ª–Ω–µ–Ω")
nb.log("  - H5 (Support/Resistance): —Ç–µ—Å—Ç –≤—ã–ø–æ–ª–Ω–µ–Ω —Å auto-identification")
nb.log("")

nb.summary_item("Phase 3.8: Regression & Validation", "‚úì –†–∞–±–æ—Ç–∞–µ—Ç", success=True)
nb.log("  - ZoneRegressionAnalyzer: duration & return models –ø–æ—Å—Ç—Ä–æ–µ–Ω—ã")
nb.log("  - ValidationSuite: out-of-sample, walk-forward, sensitivity - –≤—Å–µ —Ä–∞–±–æ—Ç–∞—é—Ç")
nb.log("")

nb.section_header("–û–±—â–∞—è –æ—Ü–µ–Ω–∫–∞")

nb.log("–§—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω–æ—Å—Ç—å:")
nb.summary_item("–í—Å–µ 67 –º–µ—Ç—Ä–∏–∫ –¥–æ—Å—Ç—É–ø–Ω—ã", "‚úì", success=True)
nb.summary_item("8 —Å—Ç—Ä–∞—Ç–µ–≥–∏–π —Ä–∞–±–æ—Ç–∞—é—Ç", "‚úì", success=True)
nb.summary_item("6 hypothesis tests", "‚úì", success=True)
nb.summary_item("Regression & Validation", "‚úì", success=True)

nb.log("")
nb.log("–ü—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å:")
nb.summary_item(f"–ü–æ–ª–Ω—ã–π –∞–Ω–∞–ª–∏–∑ {len(result.zones)} –∑–æ–Ω", f"{analysis_time:.2f} —Å–µ–∫", success=True)

nb.log("")
nb.log("Graceful Degradation:")
nb.summary_item("–û—Ç—Å—É—Ç—Å—Ç–≤–∏–µ volume", "‚úì –ö–æ—Ä—Ä–µ–∫—Ç–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞", success=True)

nb.log("")
nb.section_header("–†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏")

nb.next_steps([
    "‚úÖ –¢–µ–∫—É—â–∞—è —Ä–µ–∞–ª–∏–∑–∞—Ü–∏—è –ø–æ–ª–Ω–æ—Å—Ç—å—é —Ñ—É–Ω–∫—Ü–∏–æ–Ω–∞–ª—å–Ω–∞",
    "‚úÖ –í—Å–µ –Ω–æ–≤—ã–µ –∫–æ–º–ø–æ–Ω–µ–Ω—Ç—ã (Phases 3.3-3.8) —Ä–∞–±–æ—Ç–∞—é—Ç –∫–æ—Ä—Ä–µ–∫—Ç–Ω–æ",
    "üìä –ü—Ä–æ–≤–µ—Å—Ç–∏ —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏–µ –Ω–∞ –±–æ–ª—å—à–µ–º –∫–æ–ª–∏—á–µ—Å—Ç–≤–µ –∏–Ω—Å—Ç—Ä—É–º–µ–Ω—Ç–æ–≤",
    "üìä –û—Ü–µ–Ω–∏—Ç—å –∫–∞—á–µ—Å—Ç–≤–æ –º–æ–¥–µ–ª–µ–π –Ω–∞ out-of-sample –¥–∞–Ω–Ω—ã—Ö",
    "üîß –ù–∞ –æ—Å–Ω–æ–≤–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ —Ä–µ—à–∏—Ç—å –æ –Ω–µ–æ–±—Ö–æ–¥–∏–º–æ—Å—Ç–∏ —Ä–µ—Ñ–∞–∫—Ç–æ—Ä–∏–Ω–≥–∞",
    "üìñ –°–æ–∑–¥–∞—Ç—å –ø—Ä–∏–º–µ—Ä—ã –¥–ª—è —Ä–µ–∞–ª—å–Ω—ã—Ö —Ç–æ—Ä–≥–æ–≤—ã—Ö —Å—Ç—Ä–∞—Ç–µ–≥–∏–π"
])

nb.log("")
nb.info("–°–æ–≥–ª–∞—Å–Ω–æ TESTING_BEFORE_REFACTORING.md - –ø–µ—Ä–µ—Ö–æ–¥–∏–º –∫ Week 2: –ø—Ä–æ–¥–≤–∏–Ω—É—Ç–æ–º—É —Ç–µ—Å—Ç–∏—Ä–æ–≤–∞–Ω–∏—é")

nb.finish()

